{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d462e0a-0673-4e88-a748-8f725e48522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 3 inputs\n",
    "\n",
    "inputs = [1,2,3]\n",
    "weights = [0.2,0.8,-0.5]\n",
    "bias=2\n",
    "\n",
    "outputs=(inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cc74e8-780d-438f-b110-1e3b04310080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 4 inputs\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2,0.8,-0.5,1]\n",
    "bias = 2\n",
    "\n",
    "output=(inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76868ff-1743-4a7f-b213-d4e5d4a617d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.8, 1.21, 2.385)\n"
     ]
    }
   ],
   "source": [
    "#Layer of Neurons\n",
    "\n",
    "inputs=[1,2,3,2.5]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "weight1=weights[0]\n",
    "weight2=weights[1]\n",
    "weight3=weights[2]\n",
    "\n",
    "outputs=(inputs[0]*weight1[0]+inputs[1]*weight1[1]+inputs[2]*weight1[2]+inputs[3]*weight1[3]+biases[0],\n",
    "         inputs[0]*weight2[0]+inputs[1]*weight2[1]+inputs[2]*weight2[2]+inputs[3]*weight2[3]+biases[1],\n",
    "         inputs[0]*weight3[0]+inputs[1]*weight3[1]+inputs[2]*weight3[2]+inputs[3]*weight3[3]+biases[2])\n",
    "#Since inputs are same for the three neurons in a layer - it is the weights and biases that change\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca03ead5-7b13-4be5-820c-fa6601bb218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "#Layer of Neurons using Loops\n",
    "\n",
    "inputs=[1,2,3,2.5]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "output_list=[]\n",
    "\n",
    "for n_weights,n_bias in zip(weights,biases):\n",
    "    n_output=0\n",
    "    for n_input,weight in zip(inputs,n_weights):\n",
    "        n_output+=n_input*weight\n",
    "    n_output+=n_bias\n",
    "    output_list.append(n_output)\n",
    "\n",
    "print(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b768902-1744-488a-a361-d56afe459f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs=[1,2,3,2.5]\n",
    "\n",
    "# weights=[[0.2,0.8,-0.5,1],\n",
    "#          [0.5,-0.91,0.26,-0.5],\n",
    "#          [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "# biases=[2,3,0.5]\n",
    "\n",
    "# print(list(zip(weights,biases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5df28b-7fbf-4e04-8928-673946dddb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(zip(inputs,weights[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b883f7b7-e19a-4606-ae81-6f2db12e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neurons and Layers - Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db63a3e-cd2d-44eb-a292-169da0882bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 3 inputs - Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[1,2,3]\n",
    "weights=[0.2,0.8,-0.5]\n",
    "bias = 2\n",
    "outputs=np.dot(inputs,weights)+bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69a03c9-1b8d-48bc-91e5-5fff2ece4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.8  ]\n",
      " [1.21 ]\n",
      " [2.385]]\n"
     ]
    }
   ],
   "source": [
    "#Layer of neurons - Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1],[2],[3],[2.5]]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[[2],[3],[0.5]]\n",
    "\n",
    "outputs=np.dot(weights,inputs)+biases\n",
    "print(outputs)\n",
    "#Much more elegant than coding individually or even using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e34c904a-fbc8-48a2-ba2d-9bb37a9ba8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "#Processing batches of inputs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "outputs=np.dot(inputs,np.array(weights).T)+biases\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e50fe86-f7c2-400f-af2c-a1515e326b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "#Coding multiple layers and stacking them together\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights1=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases1=[2,3,0.5]\n",
    "\n",
    "weights2=[[0.1,-0.14,0.5],\n",
    "          [-0.5,0.12,-0.33],\n",
    "          [-0.44,0.73,-0.13]]\n",
    "\n",
    "biases2=[-1,2,-0.5]\n",
    "\n",
    "arr_inputs=np.array(inputs)\n",
    "arr_weights1=np.array(weights1)\n",
    "arr_biases1=np.array(biases1)\n",
    "arr_weights2=np.array(weights2)\n",
    "arr_biases2=np.array(biases2)\n",
    "\n",
    "outputs1=np.dot(arr_inputs,arr_weights1.T)+arr_biases1\n",
    "outputs2=np.dot(outputs1,arr_weights2.T)+arr_biases2\n",
    "print(outputs2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da2a1a0-bd73-4f34-aa4a-fd49ff645b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.outputs=np.maximum(0,inputs)\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039a8333-bbca-4c0f-a2f1-6eced9726c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "inputs=[0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "relu=Activation_ReLU()\n",
    "print(relu.forward(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c77c70d4-12a9-4c26-92c8-b92740aff5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
    "                             [0.1,0.5,0.4],\n",
    "                             [0.02,0.9,0.08]])\n",
    "class_targets = [0,1,1]\n",
    "\n",
    "relevant_values = softmax_outputs[range(len(softmax_outputs)),class_targets]\n",
    "neg_log= (-np.log(relevant_values))\n",
    "average_loss=np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e92b4d-f2cc-40b8-be13-fd4495aad322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.array([[0.7,0.1,0.2],\n",
    "                  [0.1,0.5,0.4],\n",
    "                  [0.02,0.9,0.08]])\n",
    "\n",
    "y_true=np.array([[1,0,0],\n",
    "                  [0,1,0],\n",
    "                  [0,1,0]])\n",
    "\n",
    "relevant_values=np.sum(y_pred*y_true,axis=1)\n",
    "neg_loss= (-np.log(relevant_values))\n",
    "average_loss=np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b003b86-1161-48a2-a9b6-796b56643433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, Loss : 36.0\n",
      "Iteration : 2, Loss : 33.872399999999985\n",
      "Iteration : 3, Loss : 31.870541159999995\n",
      "Iteration : 4, Loss : 29.98699217744401\n",
      "Iteration : 5, Loss : 28.21476093975706\n",
      "Iteration : 6, Loss : 26.54726856821742\n",
      "Iteration : 7, Loss : 24.978324995835766\n",
      "Iteration : 8, Loss : 23.502105988581878\n",
      "Iteration : 9, Loss : 22.113131524656684\n",
      "Iteration : 10, Loss : 20.80624545154949\n",
      "Iteration : 11, Loss : 19.576596345362915\n",
      "Iteration : 12, Loss : 18.419619501351963\n",
      "Iteration : 13, Loss : 17.331019988822064\n",
      "Iteration : 14, Loss : 16.306756707482677\n",
      "Iteration : 15, Loss : 15.343027386070442\n",
      "Iteration : 16, Loss : 14.43625446755368\n",
      "Iteration : 17, Loss : 13.583071828521266\n",
      "Iteration : 18, Loss : 12.780312283455652\n",
      "Iteration : 19, Loss : 12.024995827503426\n",
      "Iteration : 20, Loss : 11.314318574097976\n",
      "Iteration : 21, Loss : 10.645642346368787\n",
      "Iteration : 22, Loss : 10.016484883698395\n",
      "Iteration : 23, Loss : 9.424510627071816\n",
      "Iteration : 24, Loss : 8.867522049011871\n",
      "Iteration : 25, Loss : 8.34345149591527\n",
      "Iteration : 26, Loss : 7.850353512506679\n",
      "Iteration : 27, Loss : 7.386397619917536\n",
      "Iteration : 28, Loss : 6.949861520580408\n",
      "Iteration : 29, Loss : 6.539124704714106\n",
      "Iteration : 30, Loss : 6.152662434665503\n",
      "Iteration : 31, Loss : 5.789040084776769\n",
      "Iteration : 32, Loss : 5.446907815766464\n",
      "Iteration : 33, Loss : 5.124995563854669\n",
      "Iteration : 34, Loss : 4.822108326030859\n",
      "Iteration : 35, Loss : 4.537121723962434\n",
      "Iteration : 36, Loss : 4.268977830076255\n",
      "Iteration : 37, Loss : 4.016681240318748\n",
      "Iteration : 38, Loss : 3.7792953790159096\n",
      "Iteration : 39, Loss : 3.55593902211607\n",
      "Iteration : 40, Loss : 3.345783025909011\n",
      "Iteration : 41, Loss : 3.1480472490777887\n",
      "Iteration : 42, Loss : 2.9619976566572896\n",
      "Iteration : 43, Loss : 2.786943595148845\n",
      "Iteration : 44, Loss : 2.622235228675548\n",
      "Iteration : 45, Loss : 2.4672611266608238\n",
      "Iteration : 46, Loss : 2.3214459940751673\n",
      "Iteration : 47, Loss : 2.1842485358253243\n",
      "Iteration : 48, Loss : 2.055159447358047\n",
      "Iteration : 49, Loss : 1.9336995240191863\n",
      "Iteration : 50, Loss : 1.8194178821496518\n",
      "Iteration : 51, Loss : 1.7118902853146072\n",
      "Iteration : 52, Loss : 1.6107175694525138\n",
      "Iteration : 53, Loss : 1.5155241610978685\n",
      "Iteration : 54, Loss : 1.4259566831769857\n",
      "Iteration : 55, Loss : 1.3416826432012259\n",
      "Iteration : 56, Loss : 1.2623891989880334\n",
      "Iteration : 57, Loss : 1.18778199732784\n",
      "Iteration : 58, Loss : 1.1175840812857638\n",
      "Iteration : 59, Loss : 1.0515348620817762\n",
      "Iteration : 60, Loss : 0.9893891517327436\n",
      "Iteration : 61, Loss : 0.930916252865338\n",
      "Iteration : 62, Loss : 0.8758991023209965\n",
      "Iteration : 63, Loss : 0.8241334653738256\n",
      "Iteration : 64, Loss : 0.775427177570232\n",
      "Iteration : 65, Loss : 0.7295994313758314\n",
      "Iteration : 66, Loss : 0.6864801049815188\n",
      "Iteration : 67, Loss : 0.6459091307771113\n",
      "Iteration : 68, Loss : 0.6077359011481849\n",
      "Iteration : 69, Loss : 0.5718187093903269\n",
      "Iteration : 70, Loss : 0.538024223665358\n",
      "Iteration : 71, Loss : 0.5062269920467352\n",
      "Iteration : 72, Loss : 0.4763089768167732\n",
      "Iteration : 73, Loss : 0.44815911628690125\n",
      "Iteration : 74, Loss : 0.4216729125143454\n",
      "Iteration : 75, Loss : 0.3967520433847474\n",
      "Iteration : 76, Loss : 0.3733039976207088\n",
      "Iteration : 77, Loss : 0.35124173136132447\n",
      "Iteration : 78, Loss : 0.3304833450378703\n",
      "Iteration : 79, Loss : 0.3109517793461324\n",
      "Iteration : 80, Loss : 0.29257452918677557\n",
      "Iteration : 81, Loss : 0.275283374511837\n",
      "Iteration : 82, Loss : 0.2590141270781873\n",
      "Iteration : 83, Loss : 0.24370639216786646\n",
      "Iteration : 84, Loss : 0.22930334439074573\n",
      "Iteration : 85, Loss : 0.21575151673725296\n",
      "Iteration : 86, Loss : 0.20300060209808138\n",
      "Iteration : 87, Loss : 0.1910032665140845\n",
      "Iteration : 88, Loss : 0.17971497346310233\n",
      "Iteration : 89, Loss : 0.16909381853143318\n",
      "Iteration : 90, Loss : 0.159100373856225\n",
      "Iteration : 91, Loss : 0.14969754176132244\n",
      "Iteration : 92, Loss : 0.1408504170432283\n",
      "Iteration : 93, Loss : 0.13252615739597354\n",
      "Iteration : 94, Loss : 0.1246938614938715\n",
      "Iteration : 95, Loss : 0.11732445427958361\n",
      "Iteration : 96, Loss : 0.11039057903166032\n",
      "Iteration : 97, Loss : 0.10386649581088914\n",
      "Iteration : 98, Loss : 0.09772798590846545\n",
      "Iteration : 99, Loss : 0.09195226194127527\n",
      "Iteration : 100, Loss : 0.08651788326054573\n",
      "Iteration : 101, Loss : 0.08140467635984756\n",
      "Iteration : 102, Loss : 0.07659365998698067\n",
      "Iteration : 103, Loss : 0.07206697468175016\n",
      "Iteration : 104, Loss : 0.06780781647805846\n",
      "Iteration : 105, Loss : 0.06380037452420505\n",
      "Iteration : 106, Loss : 0.060029772389824425\n",
      "Iteration : 107, Loss : 0.05648201284158581\n",
      "Iteration : 108, Loss : 0.05314392588264792\n",
      "Iteration : 109, Loss : 0.05000311986298341\n",
      "Iteration : 110, Loss : 0.04704793547908098\n",
      "Iteration : 111, Loss : 0.044267402492267266\n",
      "Iteration : 112, Loss : 0.04165119900497416\n",
      "Iteration : 113, Loss : 0.03918961314378044\n",
      "Iteration : 114, Loss : 0.03687350700698295\n",
      "Iteration : 115, Loss : 0.03469428274287037\n",
      "Iteration : 116, Loss : 0.032643850632766785\n",
      "Iteration : 117, Loss : 0.030714599060370343\n",
      "Iteration : 118, Loss : 0.028899366255902458\n",
      "Iteration : 119, Loss : 0.027191413710178605\n",
      "Iteration : 120, Loss : 0.025584401159906987\n",
      "Iteration : 121, Loss : 0.02407236305135653\n",
      "Iteration : 122, Loss : 0.02264968639502141\n",
      "Iteration : 123, Loss : 0.02131108992907558\n",
      "Iteration : 124, Loss : 0.020051604514267202\n",
      "Iteration : 125, Loss : 0.018866554687474092\n",
      "Iteration : 126, Loss : 0.01775154130544445\n",
      "Iteration : 127, Loss : 0.01670242521429262\n",
      "Iteration : 128, Loss : 0.015715311884128023\n",
      "Iteration : 129, Loss : 0.014786536951776045\n",
      "Iteration : 130, Loss : 0.01391265261792606\n",
      "Iteration : 131, Loss : 0.013090414848206555\n",
      "Iteration : 132, Loss : 0.01231677133067759\n",
      "Iteration : 133, Loss : 0.011588850145034609\n",
      "Iteration : 134, Loss : 0.01090394910146302\n",
      "Iteration : 135, Loss : 0.010259525709566512\n",
      "Iteration : 136, Loss : 0.00965318774013127\n",
      "Iteration : 137, Loss : 0.009082684344689475\n",
      "Iteration : 138, Loss : 0.008545897699918259\n",
      "Iteration : 139, Loss : 0.008040835145853137\n",
      "Iteration : 140, Loss : 0.00756562178873318\n",
      "Iteration : 141, Loss : 0.0071184935410191314\n",
      "Iteration : 142, Loss : 0.006697790572744897\n",
      "Iteration : 143, Loss : 0.0063019511498957235\n",
      "Iteration : 144, Loss : 0.0059295058369368625\n",
      "Iteration : 145, Loss : 0.005579072041973895\n",
      "Iteration : 146, Loss : 0.005249348884293221\n",
      "Iteration : 147, Loss : 0.004939112365231496\n",
      "Iteration : 148, Loss : 0.0046472108244463226\n",
      "Iteration : 149, Loss : 0.004372560664721515\n",
      "Iteration : 150, Loss : 0.004114142329436494\n",
      "Iteration : 151, Loss : 0.0038709965177668067\n",
      "Iteration : 152, Loss : 0.003642220623566796\n",
      "Iteration : 153, Loss : 0.003426965384714043\n",
      "Iteration : 154, Loss : 0.0032244317304774253\n",
      "Iteration : 155, Loss : 0.003033867815206219\n",
      "Iteration : 156, Loss : 0.0028545662273275238\n",
      "Iteration : 157, Loss : 0.002685861363292454\n",
      "Iteration : 158, Loss : 0.002527126956721865\n",
      "Iteration : 159, Loss : 0.0023777737535795648\n",
      "Iteration : 160, Loss : 0.002237247324743051\n",
      "Iteration : 161, Loss : 0.0021050260078507234\n",
      "Iteration : 162, Loss : 0.001980618970786757\n",
      "Iteration : 163, Loss : 0.001863564389613244\n",
      "Iteration : 164, Loss : 0.0017534277341871227\n",
      "Iteration : 165, Loss : 0.001649800155096659\n",
      "Iteration : 166, Loss : 0.0015522969659304577\n",
      "Iteration : 167, Loss : 0.0014605562152439574\n",
      "Iteration : 168, Loss : 0.001374237342923055\n",
      "Iteration : 169, Loss : 0.0012930199159562866\n",
      "Iteration : 170, Loss : 0.0012166024389232565\n",
      "Iteration : 171, Loss : 0.0011447012347829103\n",
      "Iteration : 172, Loss : 0.0010770493918072343\n",
      "Iteration : 173, Loss : 0.0010133957727514104\n",
      "Iteration : 174, Loss : 0.0009535040825818146\n",
      "Iteration : 175, Loss : 0.0008971519913012098\n",
      "Iteration : 176, Loss : 0.0008441303086153165\n",
      "Iteration : 177, Loss : 0.0007942422073761319\n",
      "Iteration : 178, Loss : 0.0007473024929202092\n",
      "Iteration : 179, Loss : 0.0007031369155886454\n",
      "Iteration : 180, Loss : 0.0006615815238773228\n",
      "Iteration : 181, Loss : 0.0006224820558161947\n",
      "Iteration : 182, Loss : 0.0005856933663174615\n",
      "Iteration : 183, Loss : 0.0005510788883681067\n",
      "Iteration : 184, Loss : 0.0005185101260655349\n",
      "Iteration : 185, Loss : 0.0004878661776150635\n",
      "Iteration : 186, Loss : 0.00045903328651800607\n",
      "Iteration : 187, Loss : 0.0004319044192847727\n",
      "Iteration : 188, Loss : 0.00040637886810505474\n",
      "Iteration : 189, Loss : 0.0003823618770000461\n",
      "Iteration : 190, Loss : 0.00035976429006934636\n",
      "Iteration : 191, Loss : 0.00033850222052625716\n",
      "Iteration : 192, Loss : 0.0003184967392931672\n",
      "Iteration : 193, Loss : 0.0002996735820009388\n",
      "Iteration : 194, Loss : 0.000281962873304691\n",
      "Iteration : 195, Loss : 0.0002652988674923804\n",
      "Iteration : 196, Loss : 0.0002496197044235683\n",
      "Iteration : 197, Loss : 0.00023486717989213552\n",
      "Iteration : 198, Loss : 0.00022098652956051033\n",
      "Iteration : 199, Loss : 0.0002079262256634926\n",
      "Iteration : 200, Loss : 0.00019563778572677975\n",
      "Final weights :  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias :  0.6009044964039992\n"
     ]
    }
   ],
   "source": [
    "# Coding backpropagation for a single neuron\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=np.array([1.0,-2.0,3.0])\n",
    "weights=np.array([-3.0,-1.0,2.0])\n",
    "bias=1\n",
    "target_output=0.0\n",
    "learning_rate=0.001\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    linear_output=np.dot(inputs,weights)+bias\n",
    "    output=ReLU(linear_output)\n",
    "    loss=(output-target_output)**2\n",
    "\n",
    "    dloss_drelu=2*(output-target_output)\n",
    "    drelu_dlinear=derivative_relu(linear_output)\n",
    "    dlinear_dmul=1\n",
    "    dmul_dweights=inputs\n",
    "    \n",
    "    dloss_dweights=dloss_drelu*drelu_dlinear*dlinear_dmul*dmul_dweights\n",
    "    dloss_dbias=dloss_drelu*drelu_dlinear*1\n",
    "\n",
    "    weights-=learning_rate*dloss_dweights\n",
    "    bias-=learning_rate*dloss_dbias\n",
    "\n",
    "    if i%10==0:\n",
    "        print(f\"Iteration : {i+1}, Loss : {loss}\")\n",
    "\n",
    "print(\"Final weights : \",weights)\n",
    "print(\"Final bias : \",bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1becfcb-6e12-4f12-a336-4755530be526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, Loss : 466.56000000000006\n",
      "Iteration : 11, Loss : 22.06563016367191\n",
      "Iteration : 21, Loss : 5.32959636083938\n",
      "Iteration : 31, Loss : 1.4816686310919343\n",
      "Iteration : 41, Loss : 0.4119152340489983\n",
      "Iteration : 51, Loss : 0.11451559173294888\n",
      "Iteration : 61, Loss : 0.03183621207946751\n",
      "Iteration : 71, Loss : 0.008850710931419982\n",
      "Iteration : 81, Loss : 0.0024605654653895997\n",
      "Iteration : 91, Loss : 0.0006840560556525436\n",
      "Iteration : 101, Loss : 0.00019017282566014768\n",
      "Iteration : 111, Loss : 5.286950290216462e-05\n",
      "Iteration : 121, Loss : 1.4698126966450583e-05\n",
      "Iteration : 131, Loss : 4.086191934160638e-06\n",
      "Iteration : 141, Loss : 1.1359926717817246e-06\n",
      "Iteration : 151, Loss : 3.1581466831074093e-07\n",
      "Iteration : 161, Loss : 8.77988980016028e-08\n",
      "Iteration : 171, Loss : 2.4408766481731806e-08\n",
      "Iteration : 181, Loss : 6.785824135779993e-09\n",
      "Iteration : 191, Loss : 1.8865111121823123e-09\n",
      "Final weights [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
      " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
      " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
      "Final biases [-0.00698895 -0.04024714 -0.06451539]\n"
     ]
    }
   ],
   "source": [
    "#Coding backpropagation for a layer of neuron \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1,2,3,4])\n",
    "\n",
    "weights=np.array([[0.1,0.2,0.3,0.4],\n",
    "                  [0.5,0.6,0.7,0.8],\n",
    "                  [0.9,1.0,1.1,1.2]])\n",
    "\n",
    "biases=np.array([0.1,0.2,0.3])\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x>0,1,0)\n",
    "\n",
    "for i in range(200):\n",
    "    #Doing one forward pass\n",
    "    z=np.dot(weights,inputs)+biases\n",
    "    a=ReLU(z)\n",
    "    y=np.sum(a)\n",
    "\n",
    "    loss=y**2\n",
    "\n",
    "    #Backward pass\n",
    "\n",
    "    dL_dY=2*y\n",
    "    dY_da=np.ones_like(a)\n",
    "\n",
    "    dL_da=dL_dY*dY_da\n",
    "    da_dz=derivative_relu(z)\n",
    "    \n",
    "    dL_dz=dL_da*da_dz\n",
    "\n",
    "    dL_dw=np.outer(dL_dz,inputs)\n",
    "    dL_db=dL_dz\n",
    "\n",
    "    weights-=learning_rate*dL_dw\n",
    "    biases-=learning_rate*dL_db\n",
    "\n",
    "    if i%10==0:\n",
    "        print(f\"Iteration : {i+1}, Loss : {loss}\")\n",
    "\n",
    "print(\"Final weights\",weights)\n",
    "print(\"Final biases\",biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6a5af-d8cb-4fc4-826e-ef85af75d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (minor_project)",
   "language": "python",
   "name": "minor_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
