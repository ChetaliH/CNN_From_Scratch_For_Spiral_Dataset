{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d462e0a-0673-4e88-a748-8f725e48522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 3 inputs\n",
    "\n",
    "inputs = [1,2,3]\n",
    "weights = [0.2,0.8,-0.5]\n",
    "bias=2\n",
    "\n",
    "outputs=(inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cc74e8-780d-438f-b110-1e3b04310080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 4 inputs\n",
    "\n",
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2,0.8,-0.5,1]\n",
    "bias = 2\n",
    "\n",
    "output=(inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76868ff-1743-4a7f-b213-d4e5d4a617d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4.8, 1.21, 2.385)\n"
     ]
    }
   ],
   "source": [
    "#Layer of Neurons\n",
    "\n",
    "inputs=[1,2,3,2.5]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "weight1=weights[0]\n",
    "weight2=weights[1]\n",
    "weight3=weights[2]\n",
    "\n",
    "outputs=(inputs[0]*weight1[0]+inputs[1]*weight1[1]+inputs[2]*weight1[2]+inputs[3]*weight1[3]+biases[0],\n",
    "         inputs[0]*weight2[0]+inputs[1]*weight2[1]+inputs[2]*weight2[2]+inputs[3]*weight2[3]+biases[1],\n",
    "         inputs[0]*weight3[0]+inputs[1]*weight3[1]+inputs[2]*weight3[2]+inputs[3]*weight3[3]+biases[2])\n",
    "#Since inputs are same for the three neurons in a layer - it is the weights and biases that change\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca03ead5-7b13-4be5-820c-fa6601bb218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "#Layer of Neurons using Loops\n",
    "\n",
    "inputs=[1,2,3,2.5]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "output_list=[]\n",
    "\n",
    "for n_weights,n_bias in zip(weights,biases):\n",
    "    n_output=0\n",
    "    for n_input,weight in zip(inputs,n_weights):\n",
    "        n_output+=n_input*weight\n",
    "    n_output+=n_bias\n",
    "    output_list.append(n_output)\n",
    "\n",
    "print(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b768902-1744-488a-a361-d56afe459f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs=[1,2,3,2.5]\n",
    "\n",
    "# weights=[[0.2,0.8,-0.5,1],\n",
    "#          [0.5,-0.91,0.26,-0.5],\n",
    "#          [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "# biases=[2,3,0.5]\n",
    "\n",
    "# print(list(zip(weights,biases)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5df28b-7fbf-4e04-8928-673946dddb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(zip(inputs,weights[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b883f7b7-e19a-4606-ae81-6f2db12e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neurons and Layers - Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db63a3e-cd2d-44eb-a292-169da0882bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "#Neuron with 3 inputs - Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[1,2,3]\n",
    "weights=[0.2,0.8,-0.5]\n",
    "bias = 2\n",
    "outputs=np.dot(inputs,weights)+bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69a03c9-1b8d-48bc-91e5-5fff2ece4768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.8  ]\n",
      " [1.21 ]\n",
      " [2.385]]\n"
     ]
    }
   ],
   "source": [
    "#Layer of neurons - Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1],[2],[3],[2.5]]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[[2],[3],[0.5]]\n",
    "\n",
    "outputs=np.dot(weights,inputs)+biases\n",
    "print(outputs)\n",
    "#Much more elegant than coding individually or even using loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e34c904a-fbc8-48a2-ba2d-9bb37a9ba8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "#Processing batches of inputs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "outputs=np.dot(inputs,np.array(weights).T)+biases\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e50fe86-f7c2-400f-af2c-a1515e326b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n"
     ]
    }
   ],
   "source": [
    "#Coding multiple layers and stacking them together\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=[[1,2,3,2.5],[2,5,-1,2],[-1.5,2.7,3.3,-0.8]]\n",
    "\n",
    "weights1=[[0.2,0.8,-0.5,1],\n",
    "         [0.5,-0.91,0.26,-0.5],\n",
    "         [-0.26,-0.27,0.17,0.87]]\n",
    "\n",
    "biases1=[2,3,0.5]\n",
    "\n",
    "weights2=[[0.1,-0.14,0.5],\n",
    "          [-0.5,0.12,-0.33],\n",
    "          [-0.44,0.73,-0.13]]\n",
    "\n",
    "biases2=[-1,2,-0.5]\n",
    "\n",
    "arr_inputs=np.array(inputs)\n",
    "arr_weights1=np.array(weights1)\n",
    "arr_biases1=np.array(biases1)\n",
    "arr_weights2=np.array(weights2)\n",
    "arr_biases2=np.array(biases2)\n",
    "\n",
    "outputs1=np.dot(arr_inputs,arr_weights1.T)+arr_biases1\n",
    "outputs2=np.dot(outputs1,arr_weights2.T)+arr_biases2\n",
    "print(outputs2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da2a1a0-bd73-4f34-aa4a-fd49ff645b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Activation_ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.outputs=np.maximum(0,inputs)\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039a8333-bbca-4c0f-a2f1-6eced9726c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "inputs=[0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "relu=Activation_ReLU()\n",
    "print(relu.forward(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c77c70d4-12a9-4c26-92c8-b92740aff5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
    "                             [0.1,0.5,0.4],\n",
    "                             [0.02,0.9,0.08]])\n",
    "class_targets = [0,1,1]\n",
    "\n",
    "relevant_values = softmax_outputs[range(len(softmax_outputs)),class_targets]\n",
    "neg_log= (-np.log(relevant_values))\n",
    "average_loss=np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e92b4d-f2cc-40b8-be13-fd4495aad322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.array([[0.7,0.1,0.2],\n",
    "                  [0.1,0.5,0.4],\n",
    "                  [0.02,0.9,0.08]])\n",
    "\n",
    "y_true=np.array([[1,0,0],\n",
    "                  [0,1,0],\n",
    "                  [0,1,0]])\n",
    "\n",
    "relevant_values=np.sum(y_pred*y_true,axis=1)\n",
    "neg_loss= (-np.log(relevant_values))\n",
    "average_loss=np.mean(neg_loss)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b003b86-1161-48a2-a9b6-796b56643433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, Loss : 36.0\n",
      "Iteration : 11, Loss : 19.576596345362915\n",
      "Iteration : 21, Loss : 10.645642346368787\n",
      "Iteration : 31, Loss : 5.789040084776769\n",
      "Iteration : 41, Loss : 3.1480472490777887\n",
      "Iteration : 51, Loss : 1.7118902853146072\n",
      "Iteration : 61, Loss : 0.930916252865338\n",
      "Iteration : 71, Loss : 0.5062269920467352\n",
      "Iteration : 81, Loss : 0.275283374511837\n",
      "Iteration : 91, Loss : 0.14969754176132244\n",
      "Iteration : 101, Loss : 0.08140467635984756\n",
      "Iteration : 111, Loss : 0.044267402492267266\n",
      "Iteration : 121, Loss : 0.02407236305135653\n",
      "Iteration : 131, Loss : 0.013090414848206555\n",
      "Iteration : 141, Loss : 0.0071184935410191314\n",
      "Iteration : 151, Loss : 0.0038709965177668067\n",
      "Iteration : 161, Loss : 0.0021050260078507234\n",
      "Iteration : 171, Loss : 0.0011447012347829103\n",
      "Iteration : 181, Loss : 0.0006224820558161947\n",
      "Iteration : 191, Loss : 0.00033850222052625716\n",
      "Final weights :  [-3.3990955  -0.20180899  0.80271349]\n",
      "Final bias :  0.6009044964039992\n"
     ]
    }
   ],
   "source": [
    "# Coding backpropagation for a single neuron\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=np.array([1.0,-2.0,3.0])\n",
    "weights=np.array([-3.0,-1.0,2.0])\n",
    "bias=1\n",
    "target_output=0.0\n",
    "learning_rate=0.001\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i in range(200):\n",
    "    \n",
    "    linear_output=np.dot(inputs,weights)+bias\n",
    "    output=ReLU(linear_output)\n",
    "    loss=(output-target_output)**2\n",
    "\n",
    "    dloss_drelu=2*(output-target_output)\n",
    "    drelu_dlinear=derivative_relu(linear_output)\n",
    "    dlinear_dmul=1\n",
    "    dmul_dweights=inputs\n",
    "    \n",
    "    dloss_dweights=dloss_drelu*drelu_dlinear*dlinear_dmul*dmul_dweights\n",
    "    dloss_dbias=dloss_drelu*drelu_dlinear*1\n",
    "\n",
    "    weights-=learning_rate*dloss_dweights\n",
    "    bias-=learning_rate*dloss_dbias\n",
    "\n",
    "    if i%10==0:\n",
    "        print(f\"Iteration : {i+1}, Loss : {loss}\")\n",
    "\n",
    "print(\"Final weights : \",weights)\n",
    "print(\"Final bias : \",bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1becfcb-6e12-4f12-a336-4755530be526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, Loss : 466.56000000000006\n",
      "Iteration : 11, Loss : 22.06563016367191\n",
      "Iteration : 21, Loss : 5.32959636083938\n",
      "Iteration : 31, Loss : 1.4816686310919343\n",
      "Iteration : 41, Loss : 0.4119152340489983\n",
      "Iteration : 51, Loss : 0.11451559173294888\n",
      "Iteration : 61, Loss : 0.03183621207946751\n",
      "Iteration : 71, Loss : 0.008850710931419982\n",
      "Iteration : 81, Loss : 0.0024605654653895997\n",
      "Iteration : 91, Loss : 0.0006840560556525436\n",
      "Iteration : 101, Loss : 0.00019017282566014768\n",
      "Iteration : 111, Loss : 5.286950290216462e-05\n",
      "Iteration : 121, Loss : 1.4698126966450583e-05\n",
      "Iteration : 131, Loss : 4.086191934160638e-06\n",
      "Iteration : 141, Loss : 1.1359926717817246e-06\n",
      "Iteration : 151, Loss : 3.1581466831074093e-07\n",
      "Iteration : 161, Loss : 8.77988980016028e-08\n",
      "Iteration : 171, Loss : 2.4408766481731806e-08\n",
      "Iteration : 181, Loss : 6.785824135779993e-09\n",
      "Iteration : 191, Loss : 1.8865111121823123e-09\n",
      "Final weights [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
      " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
      " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
      "Final biases [-0.00698895 -0.04024714 -0.06451539]\n"
     ]
    }
   ],
   "source": [
    "#Coding backpropagation for a layer of neuron \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1,2,3,4])\n",
    "\n",
    "weights=np.array([[0.1,0.2,0.3,0.4],\n",
    "                  [0.5,0.6,0.7,0.8],\n",
    "                  [0.9,1.0,1.1,1.2]])\n",
    "\n",
    "biases=np.array([0.1,0.2,0.3])\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x>0,1,0)\n",
    "\n",
    "for i in range(200):\n",
    "    #Doing one forward pass\n",
    "    z=np.dot(weights,inputs)+biases\n",
    "    a=ReLU(z)\n",
    "    y=np.sum(a)\n",
    "\n",
    "    loss=y**2\n",
    "\n",
    "    #Backward pass\n",
    "\n",
    "    dL_dY=2*y\n",
    "    dY_da=np.ones_like(a)\n",
    "\n",
    "    dL_da=dL_dY*dY_da\n",
    "    da_dz=derivative_relu(z)\n",
    "    \n",
    "    dL_dz=dL_da*da_dz\n",
    "\n",
    "    dL_dw=np.outer(dL_dz,inputs)\n",
    "    dL_db=dL_dz\n",
    "\n",
    "    weights-=learning_rate*dL_dw\n",
    "    biases-=learning_rate*dL_db\n",
    "\n",
    "    if i%10==0:\n",
    "        print(f\"Iteration : {i+1}, Loss : {loss}\")\n",
    "\n",
    "print(\"Final weights\",weights)\n",
    "print(\"Final biases\",biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6a5af-d8cb-4fc4-826e-ef85af75d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (minor_project)",
   "language": "python",
   "name": "minor_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
